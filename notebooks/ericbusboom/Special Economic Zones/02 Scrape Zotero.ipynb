{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892d5620-4acf-42af-96e8-fa18b6559704",
   "metadata": {},
   "source": [
    "# Fetching annotations from Zotero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e272aa7b-6524-45f6-b7e3-9c7799593bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyzotero import zotero\n",
    "from pathlib import Path\n",
    "from pyzotero.zotero_errors import ResourceNotFound\n",
    "from dataclasses import asdict\n",
    "import fitz \n",
    "from pdf_annot.extract import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e690d9-cd08-4868-8e87-80e1449cf06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Credentials, which, BTW, are not included in the GIT repo ... \n",
    "# You'll have to get your own. \n",
    "\n",
    "with open('credentials.yaml') as f:\n",
    "    cred = yaml.safe_load(f)\n",
    "    \n",
    "# Get all of the items from the group\n",
    "\n",
    "zot = zotero.Zotero(cred['userId'], 'group', cred['key'])\n",
    "\n",
    "items = { item['data']['key']:item for item in zot.everything(zot.items()) }\n",
    "attach = { k:v for k,v in items.items() if v['data'].get('contentType') }\n",
    "\n",
    "assert zot.count_items() == len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8013939f-3353-4025-aa82-fac0b7024518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch: special-economic-zones-progress-emerging-challenges-and-future-directions.html\n",
      "\n",
      "Code: 404\n",
      "URL: https://api.zotero.org/groups/3865474/items/AXKHW636/file\n",
      "Method: GET\n",
      "Response: Not found\n",
      "AXKHW636 <class 'RuntimeError'> cannot open pdf/special-economic-zones-progress-emerging-challenges-and-future-directions.html.zip: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Fetch each of the PDFS, but only if they aren't already cached \n",
    "# Extract annotations and text and pages\n",
    "\n",
    "fitz.TOOLS.mupdf_display_errors(False) # Turn off \"mupdf: invalid page object\" warnings\n",
    "\n",
    "for i, (key, item) in enumerate(items.items()):\n",
    "    try:\n",
    "        key = item['data']['key']\n",
    "        typ = item['data']['itemType']\n",
    "        ct = item['data'].get('contentType')\n",
    "    except Exception as e:\n",
    "        print(key, e)\n",
    "        continue \n",
    "        \n",
    "    if  ct and typ == 'attachment':\n",
    "        fn = item['data']['filename']\n",
    "        p = Path(f'pdf/{fn}')\n",
    "        \n",
    "        if ct == 'text/html':\n",
    "            p = Path(f'pdf/{fn}.zip')\n",
    "        else:\n",
    "            p = Path(f'pdf/{fn}')\n",
    "        \n",
    "        if not p.exists():\n",
    "            print(f\"Fetch: {fn}\")\n",
    "            try:\n",
    "                zot.dump(key, path='pdf')\n",
    "            except ResourceNotFound as e:\n",
    "                print(e)\n",
    "            \n",
    "        item['_path'] = str(p)\n",
    "        \n",
    "        try:\n",
    "            item['_annotations'] = [asdict(e) for e in annotations(p)]\n",
    "        except TypeError as e:\n",
    "            pass # annotations didn't return dataclass to asdict\n",
    "        except Exception as e:\n",
    "            print(key, type(e), e)\n",
    "        \n",
    "        if(p.exists()):\n",
    "            doc = fitz.open(p) \n",
    "\n",
    "            item['_pages'] = [{'pageno': i,  'text': page.get_text(\"text\"), 'html': None }\n",
    "                          for i, page in enumerate(doc)]\n",
    "        else:\n",
    "            item['_pages'] =  None\n",
    "\n",
    "        \n",
    "    else:\n",
    "        item['_pages'] =  None\n",
    "        item['_path'] = None\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2eb6d6-6e27-4767-9484-cf3d86fd07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data as JSON to a ZIP file\n",
    "import json \n",
    "import gzip\n",
    "\n",
    "with open('zotero_records.json', 'w') as f:\n",
    "    f.write(json.dumps(items))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b388e61-d427-4def-be4d-8de3e4900d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'CATZXAXBKFWVCA5Z',\n",
       "  'HostId': 'BITsdywmLc+BThRwvy90Aau2IZA18pr9osMN2EoptBCw51iQioEWg8686X3etTpRva/1NbYsc+s=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'BITsdywmLc+BThRwvy90Aau2IZA18pr9osMN2EoptBCw51iQioEWg8686X3etTpRva/1NbYsc+s=',\n",
       "   'x-amz-request-id': 'CATZXAXBKFWVCA5Z',\n",
       "   'date': 'Mon, 16 Aug 2021 22:30:44 GMT',\n",
       "   'etag': '\"b4e84bf5a4581fecece895c3eca61f95\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"b4e84bf5a4581fecece895c3eca61f95\"'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the file into S3\n",
    "import boto3\n",
    "import zlib\n",
    "\n",
    "session = boto3.Session(profile_name=cred['s3profile'])\n",
    "client = session.client('s3')\n",
    "  \n",
    "k = 'sandiegodata.org/sez_zotero_papers/zotero_records-1.0.3.json.gzip'\n",
    "    \n",
    "client.put_object(Body=zlib.compress(json.dumps(items).encode('utf-8')), \n",
    "                  Bucket='ds.civicknowledge.org', \n",
    "                  Key=k, \n",
    "                  ACL='public-read')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef572669-9b3b-49a1-874d-a863d1e36a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this for later, incase we discover that the text is garbled. \n",
    "\n",
    "from operator import itemgetter \n",
    "from itertools import groupby \n",
    "\n",
    "def recover(words, rect):\n",
    "    \"\"\" Word recovery.\n",
    "\n",
    "    Notes:\n",
    "        Method 'get_textWords()' does not try to recover words, if their single\n",
    "        letters do not appear in correct lexical order. This function steps in\n",
    "        here and creates a new list of recovered words.\n",
    "    Args:\n",
    "        words: list of words as created by 'get_textWords()'\n",
    "        rect: rectangle to consider (usually the full page)\n",
    "    Returns:\n",
    "        List of recovered words. Same format as 'get_text_words', but left out\n",
    "        block, line and word number - a list of items of the following format:\n",
    "        [x0, y0, x1, y1, \"word\"]\n",
    "    \"\"\"\n",
    "    # build my sublist of words contained in given rectangle\n",
    "    mywords = [w for w in words if fitz.Rect(w[:4]) in rect]\n",
    "\n",
    "    # sort the words by lower line, then by word start coordinate\n",
    "    mywords.sort(key=itemgetter(3, 0))  # sort by y1, x0 of word rectangle\n",
    "\n",
    "    # build word groups on same line\n",
    "    grouped_lines = groupby(mywords, key=itemgetter(3))\n",
    "\n",
    "    words_out = []  # we will return this\n",
    "\n",
    "    # iterate through the grouped lines\n",
    "    # for each line coordinate (\"_\"), the list of words is given\n",
    "    for _, words_in_line in grouped_lines:\n",
    "        for i, w in enumerate(words_in_line):\n",
    "            if i == 0:  # store first word\n",
    "                x0, y0, x1, y1, word = w[:5]\n",
    "                continue\n",
    "\n",
    "            r = fitz.Rect(w[:4])  # word rect\n",
    "\n",
    "            # Compute word distance threshold as 20% of width of 1 letter.\n",
    "            # So we should be safe joining text pieces into one word if they\n",
    "            # have a distance shorter than that.\n",
    "            threshold = r.width / len(w[4]) / 5\n",
    "            if r.x0 <= x1 + threshold:  # join with previous word\n",
    "                word += w[4]  # add string\n",
    "                x1 = r.x1  # new end-of-word coordinate\n",
    "                y0 = max(y0, r.y0)  # extend word rect upper bound\n",
    "                continue\n",
    "\n",
    "            # now have a new word, output previous one\n",
    "            words_out.append([x0, y0, x1, y1, word])\n",
    "\n",
    "            # store the new word\n",
    "            x0, y0, x1, y1, word = w[:5]\n",
    "\n",
    "        # output word waiting for completion\n",
    "        words_out.append([x0, y0, x1, y1, word])\n",
    "\n",
    "    return words_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98247fa3-0f95-4424-8b1f-821a9573da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
